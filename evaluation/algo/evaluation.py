
import gc
from hashlib import algorithms_available
import json
import torch
from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer
import torch.nn.functional as F
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from collections import Counter
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from typing import Dict, List, Optional
import os
from datetime import datetime


def calculate_ppl(model:AutoModelForCausalLM,tokenizer:AutoTokenizer,text:str):
    """
    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å›°æƒ‘åº¦ï¼ˆPPLï¼‰ã€‚
    Args:
        model:AutoModelForCausalLM: è¯„ä¼°æ¨¡å‹
        tokenizer:AutoTokenizer: è¯„ä¼°æ¨¡å‹åˆ†è¯å™¨
        text:str: éœ€è¦è®¡ç®—å›°æƒ‘åº¦çš„æ–‡æœ¬ã€‚
        
    Returns:
        float: å›°æƒ‘åº¦åˆ†æ•°ã€‚åˆ†æ•°è¶Šä½è¡¨ç¤ºæ–‡æœ¬è¶Šè‡ªç„¶ã€‚
    """
    with torch.no_grad():
        # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†
        tokenizer_output = tokenizer(text, return_tensors="pt", add_special_tokens=False)
        encoded_text = tokenizer_output["input_ids"][0].to(model.device)
        del tokenizer_output  # ç«‹å³æ¸…ç†tokenizerè¾“å‡º
        
        # è®¾ç½®æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰
        criterion = torch.nn.CrossEntropyLoss()
        
        # è·å–æ¨¡å‹é¢„æµ‹ç»“æœ
        model_input = torch.unsqueeze(encoded_text, 0)
        model_output = model(model_input, return_dict=True)
        logits = model_output.logits[0].clone()  # ä½¿ç”¨cloneé¿å…å¼•ç”¨
        
        # ç«‹å³æ¸…ç†æ¨¡å‹è¾“å‡ºå’Œè¾“å…¥
        del model_output, model_input
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # è®¡ç®—æŸå¤±å’Œå›°æƒ‘åº¦
        # æˆ‘ä»¬é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œæ‰€ä»¥æ¯”è¾ƒlogits[:-1]ä¸encoded_text[1:]
        logits_slice = logits[:-1]
        target_slice = encoded_text[1:]
        loss = criterion(logits_slice, target_slice)
        ppl = torch.exp(loss)  # å›°æƒ‘åº¦æ˜¯æŸå¤±çš„æŒ‡æ•°
        # ä¿å­˜ç»“æœ
        result = ppl.item()
        return result
def calculate_semantic_entropy(model:AutoModelForCausalLM,tokenizer:AutoTokenizer,text:str):
    """
    è®¡ç®—ç»™å®šæ–‡æœ¬çš„è¯­ä¹‰ç†µã€‚
    Args:
        model:AutoModelForCausalLM: è¯„ä¼°æ¨¡å‹
        tokenizer:AutoTokenizer: è¯„ä¼°æ¨¡å‹åˆ†è¯å™¨
        text:str: éœ€è¦è®¡ç®—è¯­ä¹‰ç†µçš„æ–‡æœ¬ã€‚
        
    Returns:
        float: è¯­ä¹‰ç†µåˆ†æ•°ã€‚åˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¯­ä¹‰ä¸ç¡®å®šæ€§è¶Šå¤§ã€‚
    """
    with torch.no_grad():
        # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†
        tokenizer_output = tokenizer(text, return_tensors="pt", add_special_tokens=False)
        encoded_text = tokenizer_output["input_ids"][0].to(model.device)
        # è·å–æ¨¡å‹é¢„æµ‹ç»“æœ
        model_input = torch.unsqueeze(encoded_text, 0)
        model_output = model(model_input, return_dict=True)
        logits = model_output.logits[0].clone()  # ä½¿ç”¨cloneé¿å…å¼•ç”¨
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        # å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        logits_slice = logits[:-1]  # å¯¹é™¤æœ€åä¸€ä¸ªä½ç½®å¤–çš„æ‰€æœ‰ä½ç½®
        probabilities = F.softmax(logits_slice, dim=-1)
        log_probs = torch.log(probabilities + 1e-10)  # æ·»åŠ å°å¸¸æ•°é¿å…log(0)
        entropy_per_position = -torch.sum(probabilities * log_probs, dim=-1)
        # è®¡ç®—å¹³å‡è¯­ä¹‰ç†µ
        semantic_entropy = torch.mean(entropy_per_position)
        # ä¿å­˜ç»“æœ
        result = semantic_entropy.item()
        return result


def calculate_rouge1(reference, candidate):
    """
    è®¡ç®—ROUGE-1åˆ†æ•°ã€‚
    
    ROUGE-1æ˜¯åŸºäºå•ä¸ªè¯ï¼ˆunigramï¼‰åŒ¹é…çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å€™é€‰æ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬çš„ç›¸ä¼¼åº¦ã€‚
    åˆ†æ•°èŒƒå›´ä»0åˆ°1ï¼Œ1è¡¨ç¤ºå®Œå…¨åŒ¹é…ã€‚
    
    å‚æ•°ï¼š
        reference (str): å‚è€ƒæ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯åŸå§‹æ–‡æœ¬ï¼‰ã€‚
        candidate (str): å€™é€‰æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯ç”Ÿæˆæˆ–åŠ æ°´å°çš„æ–‡æœ¬ï¼‰ã€‚
        
    è¿”å›ï¼š
        dict: åŒ…å«ROUGE-1çš„ç²¾ç¡®ç‡(precision)ã€å¬å›ç‡(recall)å’ŒF1å€¼ã€‚
    """
    # ç¡®ä¿nltkå·²ä¸‹è½½å¿…è¦çš„èµ„æº
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt_tab')
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
        
    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    reference_tokens = nltk.word_tokenize(reference.lower())
    candidate_tokens = nltk.word_tokenize(candidate.lower())
    
    # è®¡ç®—unigrams
    reference_unigrams = Counter(reference_tokens)
    candidate_unigrams = Counter(candidate_tokens)
    
    # è®¡ç®—å…±åŒå‡ºç°çš„unigrams
    common_unigrams = reference_unigrams & candidate_unigrams
    common_count = sum(common_unigrams.values())
    
    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼
    candidate_total = sum(candidate_unigrams.values())
    reference_total = sum(reference_unigrams.values())
    precision = common_count / max(candidate_total, 1)
    recall = common_count / max(reference_total, 1)
    f1 = 2 * precision * recall / max(precision + recall, 1e-10)
    
    # ä¿å­˜ç»“æœ
    result = {
        'precision': precision,
        'recall': recall,
        'f1': f1
    }
    
    return result

def calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):
    """
    è®¡ç®—BLEUåˆ†æ•°ã€‚

    å‚æ•°ï¼š
        reference (str): å‚è€ƒæ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯åŸå§‹æ–‡æœ¬ï¼‰ã€‚
        candidate (str): å€™é€‰æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯ç”Ÿæˆæˆ–åŠ æ°´å°çš„æ–‡æœ¬ï¼‰ã€‚
        weights (tuple): n-gramæƒé‡ï¼Œé»˜è®¤ä¸º(0.25, 0.25, 0.25, 0.25)ï¼Œè¡¨ç¤ºBLEU-4ã€‚
        
    è¿”å›ï¼š
        float: BLEUåˆ†æ•°ã€‚
    """
    # ç¡®ä¿nltkå·²ä¸‹è½½å¿…è¦çš„èµ„æº
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt_tab')
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
        
    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    reference_tokens = nltk.word_tokenize(reference.lower())
    candidate_tokens = nltk.word_tokenize(candidate.lower())
    
    # ä½¿ç”¨å¹³æ»‘å‡½æ•°é¿å…é›¶ç²¾ç¡®ç‡
    smoothing = SmoothingFunction().method1
    
    # è®¡ç®—BLEUåˆ†æ•°
    bleu_score = sentence_bleu(
        [reference_tokens],  # å‚è€ƒæ–‡æœ¬åˆ—è¡¨ï¼ˆå¯ä»¥æœ‰å¤šä¸ªå‚è€ƒï¼‰
        candidate_tokens,    # å€™é€‰æ–‡æœ¬
        weights=weights,     # n-gramæƒé‡
        smoothing_function=smoothing  # å¹³æ»‘å‡½æ•°
    )
    # ä¿å­˜ç»“æœ
    result = bleu_score
    return result

def calculate_lexical_diversity(text):
    """
    è®¡ç®—ç»™å®šæ–‡æœ¬çš„è¯æ±‡ä¸°å¯Œåº¦ï¼Œä¸ä¾èµ–å‚è€ƒæ–‡æœ¬ã€‚

    å‚æ•°:
        text (str): éœ€è¦è¯„ä¼°çš„æ–‡æœ¬ã€‚
        
    è¿”å›:
        dict: åŒ…å«å¤šç§è¯æ±‡ä¸°å¯Œåº¦æŒ‡æ ‡çš„å­—å…¸ã€‚
    """
    try:
        # ç¡®ä¿nltkçš„åˆ†è¯å™¨å¯ç”¨
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')

    # å°†æ–‡æœ¬åˆ†è¯
    tokens = nltk.word_tokenize(text.lower())
    
    # è¿‡æ»¤æ‰éå­—æ¯çš„tokenï¼Œä¾‹å¦‚æ ‡ç‚¹ç¬¦å·
    words = [word for word in tokens if word.isalpha()]
    
    if not words:
        # æ¸…ç†ä¸´æ—¶å˜é‡
        del tokens, words
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        gc.collect()
        return {
            'ttr': 0,
            'rttr': 0,
            'unigram_entropy': 0
        }

    total_tokens = len(words)
    unique_words = set(words)
    unique_tokens = len(unique_words)

    # 1. è®¡ç®— TTR (Type-Token Ratio)
    ttr = unique_tokens / total_tokens if total_tokens > 0 else 0

    # 2. è®¡ç®— RTTR (Root TTR)
    rttr = unique_tokens / np.sqrt(total_tokens) if total_tokens > 0 else 0

    # 3. è®¡ç®— Unigram Entropy
    counts = Counter(words)
    probabilities = [count / total_tokens for count in counts.values()]
    entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])
    
    # ä¿å­˜ç»“æœ
    result = {
        'ttr': ttr,
        'rttr': rttr,
        'unigram_entropy': entropy
    }
    return result


def parse_conversation(model:AutoModelForCausalLM,tokenizer:AutoTokenizer,conversation_path:str,result_path:str):
    with open(conversation_path, 'r', encoding='utf-8') as file:
        conversation = json.load(file)
    result = {
        "conversation_info":conversation_path,
        "experiment_config":{
            "steganography_algorithm":conversation['sessionInfo']['steganographyAlgorithm'],
            "question_domain":conversation['sessionInfo']['topic'],
            "question_index":conversation['sessionInfo']['questionIndex']
        },
        "average_capacity_metrics":{
            "bits_per_round":0,  # å°†åŠ¨æ€è®¡ç®—
            "round_per_bit":0,   # å°†åŠ¨æ€è®¡ç®—
            "total_bits_transmitted":0,  # å®é™…ä¼ è¾“çš„æ€»æ¯”ç‰¹æ•°
            "bits_per_round_list":[]     # æ¯è½®ä¼ è¾“çš„æ¯”ç‰¹æ•°åˆ—è¡¨
        },
        "rounds":[
        ],
        "average_quality_metrics":None
    }
    print(f"è§£æå¯¹è¯ï¼š{conversation_path}")
    print("å®éªŒé…ç½®")
    print(f"    éšå†™ç®—æ³•ï¼š{conversation['sessionInfo']['steganographyAlgorithm']}")
    print(f"    å¯¹è¯é¢†åŸŸï¼š{conversation['sessionInfo']['topic']}")
    print(f"    é—®é¢˜ç¼–å·ï¼š{conversation['sessionInfo']['questionIndex']}")
    print("å¹³å‡ä¼ è¾“æ•ˆç‡æŒ‡æ ‡:")
    print(f"    å¹³å‡æ¯è½®å¯åµŒå…¥æ¯”ç‰¹æ•°(bits/round):{result['average_capacity_metrics']['bits_per_round']}")
    print(f"    å¹³å‡æ¯æ¯”ç‰¹å¯åµŒå…¥è½®æ•°(round/bit):{result['average_capacity_metrics']['round_per_bit']}")
    print("é€è½®æ–‡æœ¬è´¨é‡åˆ†æ:")
    ppl_list = []
    entropy_list = []
    rouge1_list = []
    bleu_list = []
    lex_div_list = []
    bits_per_round_list = []
    rounds = conversation['rounds']
    
    # éªŒè¯è½®æ•°ï¼šå›ºå®šä¸º5è½®
    EXPECTED_ROUNDS = 5
    if len(rounds) != EXPECTED_ROUNDS:
        print(f"    è­¦å‘Š: å¯¹è¯è½®æ•°ä¸º {len(rounds)}ï¼ŒæœŸæœ›ä¸º {EXPECTED_ROUNDS} è½®")
        if len(rounds) > EXPECTED_ROUNDS:
            print(f"    å°†åªå¤„ç†å‰ {EXPECTED_ROUNDS} è½®æ•°æ®")
            rounds = rounds[:EXPECTED_ROUNDS]
        elif len(rounds) < EXPECTED_ROUNDS:
            print(f"    æ•°æ®ä¸å®Œæ•´ï¼Œåªæœ‰ {len(rounds)} è½®")
    
    for round in rounds:
        stego_text = round['clientTurn']['publicCarrierMessage']
        cover_text = round['clientTurn']['normalMessage']
        
        # è®¡ç®—å®é™…ä¼ è¾“çš„æ¯”ç‰¹æ•°ï¼ˆä»payloadä¸­è·å–ï¼‰
        try:
            import base64
            payload_base64 = round['clientTurn']['covertPacket']['payload_base64']
            payload_bits = base64.b64decode(payload_base64).decode('utf-8')
            bits_transmitted = len(payload_bits)
        except (KeyError, Exception) as e:
            # å¦‚æœæ— æ³•è·å–payloadï¼Œä½¿ç”¨ä¼°ç®—å€¼
            bits_transmitted = 1.5  # é»˜è®¤ä¼°ç®—å€¼
            print(f"    è­¦å‘Š: æ— æ³•è·å–ç¬¬{round['roundNumber']}è½®çš„payloadæ•°æ®ï¼Œä½¿ç”¨ä¼°ç®—å€¼: {e}")
        
        bits_per_round_list.append(bits_transmitted)
        
        ppl = calculate_ppl(model,tokenizer,stego_text)
        entropy = calculate_semantic_entropy(model,tokenizer,stego_text)
        rouge1 = calculate_rouge1(cover_text,stego_text)
        bleu = calculate_bleu(cover_text,stego_text)
        lex_div = calculate_lexical_diversity(stego_text)
        print(f"    è½®æ¬¡ï¼š{round['roundNumber']}")
        print(f"        ä¼ è¾“æ¯”ç‰¹æ•°:{bits_transmitted}")
        print(f"        å›°æƒ‘åº¦:{ppl}")
        print(f"        è¯­ä¹‰ç†µ:{entropy}")
        print(f"        ROUGE-1(Precision):{rouge1['precision']}")
        print(f"        ROUGE-1(Recall):{rouge1['recall']}")
        print(f"        ROUGE-1(F1):{rouge1['f1']}")
        print(f"        BLEU:{bleu}")
        print(f"        è¯æ±‡ä¸°å¯Œåº¦(TTR):{lex_div['ttr']}")
        print(f"        è¯æ±‡ä¸°å¯Œåº¦(RTTR):{lex_div['rttr']}")
        print(f"        è¯æ±‡ä¸°å¯Œåº¦(Unigram Entropy):{lex_div['unigram_entropy']}")
        result['rounds'].append({
            "round_number":round['roundNumber'],
            "bits_transmitted":bits_transmitted,
            "ppl":ppl,
            "entropy":entropy,
            "rouge1_precision":rouge1['precision'],
            "rouge1_recall":rouge1['recall'],
            "rouge1_f1":rouge1['f1'],
            "bleu":bleu,
            "lex_div_ttr":lex_div['ttr'],
            "lex_div_rttr":lex_div['rttr'],
            "lex_div_unigram_entropy":lex_div['unigram_entropy']
        })
        ppl_list.append(ppl)
        entropy_list.append(entropy)
        rouge1_list.append(rouge1)
        bleu_list.append(bleu)
        lex_div_list.append(lex_div)

    # æ›´æ–°ä¼ è¾“å®¹é‡æŒ‡æ ‡
    total_bits_transmitted = sum(bits_per_round_list)
    avg_bits_per_round = np.mean(bits_per_round_list) if bits_per_round_list else 0
    avg_round_per_bit = 1.0 / avg_bits_per_round if avg_bits_per_round > 0 else 0
    
    result['average_capacity_metrics']['bits_per_round'] = avg_bits_per_round
    result['average_capacity_metrics']['round_per_bit'] = avg_round_per_bit
    result['average_capacity_metrics']['total_bits_transmitted'] = total_bits_transmitted
    result['average_capacity_metrics']['bits_per_round_list'] = bits_per_round_list
    
    print("æ›´æ–°åçš„ä¼ è¾“æ•ˆç‡æŒ‡æ ‡:")
    print(f"    å®é™…å¹³å‡æ¯è½®å¯åµŒå…¥æ¯”ç‰¹æ•°(bits/round):{avg_bits_per_round:.3f}")
    print(f"    å®é™…å¹³å‡æ¯æ¯”ç‰¹å¯åµŒå…¥è½®æ•°(round/bit):{avg_round_per_bit:.3f}")
    print(f"    å®é™…æ€»ä¼ è¾“æ¯”ç‰¹æ•°:{total_bits_transmitted:.1f}")
    
    print("å¹³å‡æ–‡æœ¬è´¨é‡æŒ‡æ ‡:")
    print(f"    å¹³å‡å›°æƒ‘åº¦:{np.mean(ppl_list)}")
    print(f"    å¹³å‡è¯­ä¹‰ç†µ:{np.mean(entropy_list)}")
    print(f"    å¹³å‡ROUGE-1(Precision):{np.mean([rouge1['precision'] for rouge1 in rouge1_list])}")
    print(f"    å¹³å‡ROUGE-1(Recall):{np.mean([rouge1['recall'] for rouge1 in rouge1_list])}")
    print(f"    å¹³å‡ROUGE-1(F1):{np.mean([rouge1['f1'] for rouge1 in rouge1_list])}")
    print(f"    å¹³å‡BLEU:{np.mean(bleu_list)}")
    print(f"    å¹³å‡è¯æ±‡ä¸°å¯Œåº¦(TTR):{np.mean([lex_div['ttr'] for lex_div in lex_div_list])}")
    print(f"    å¹³å‡è¯æ±‡ä¸°å¯Œåº¦(RTTR):{np.mean([lex_div['rttr'] for lex_div in lex_div_list])}")
    print(f"    å¹³å‡è¯æ±‡ä¸°å¯Œåº¦(Unigram Entropy):{np.mean([lex_div['unigram_entropy'] for lex_div in lex_div_list])}")
    result['average_quality_metrics'] = {
        "ppl":np.mean(ppl_list),
        "entropy":np.mean(entropy_list),
        "rouge1_precision":np.mean([rouge1['precision'] for rouge1 in rouge1_list]),
        "rouge1_recall":np.mean([rouge1['recall'] for rouge1 in rouge1_list]),
        "rouge1_f1":np.mean([rouge1['f1'] for rouge1 in rouge1_list]),
        "bleu":np.mean(bleu_list),
        "lex_div_ttr":np.mean([lex_div['ttr'] for lex_div in lex_div_list]),
        "lex_div_rttr":np.mean([lex_div['rttr'] for lex_div in lex_div_list]),
        "lex_div_unigram_entropy":np.mean([lex_div['unigram_entropy'] for lex_div in lex_div_list])
    }
    result_file_name = f"{result_path}/evaluation_{conversation_path.split('/')[-1]}.json"
    with open(result_file_name,'w',encoding='utf-8') as file:
        json.dump(result,file)
    print(f"ç»“æœå·²ä¿å­˜åˆ°ï¼š{result_file_name}")


class EvaluationVisualizer:
    """
    è¯„ä¼°æŒ‡æ ‡å¯è§†åŒ–ç±»
    ç”¨äºå±•ç¤ºå’Œåˆ†æéšå†™æ–‡æœ¬çš„å„ç§è´¨é‡æŒ‡æ ‡
    """
    
    def __init__(self, style='seaborn-v0_8', figsize=(12, 8)):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            style: matplotlibæ ·å¼
            figsize: å›¾è¡¨å¤§å°
        """
        plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼ï¼Œé¿å…seabornç‰ˆæœ¬é—®é¢˜
        sns.set_palette("husl")
        self.figsize = figsize
        self.colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 
                      '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
    
    def plot_quality_metrics(self, evaluation_data: Dict, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶æ–‡æœ¬è´¨é‡æŒ‡æ ‡å›¾è¡¨
        
        Args:
            evaluation_data: è¯„ä¼°æ•°æ®å­—å…¸
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾è¡¨
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('éšå†™æ–‡æœ¬è´¨é‡æŒ‡æ ‡åˆ†æ', fontsize=16, fontweight='bold')
        
        rounds_data = evaluation_data['rounds']
        round_numbers = [r['round_number'] for r in rounds_data]
        
        # 1. å›°æƒ‘åº¦ (PPL)
        ppls = [r['ppl'] for r in rounds_data]
        axes[0, 0].plot(round_numbers, ppls, marker='o', linewidth=2, markersize=6, color=self.colors[0])
        axes[0, 0].set_title('å›°æƒ‘åº¦ (PPL)', fontweight='bold')
        axes[0, 0].set_xlabel('è½®æ¬¡')
        axes[0, 0].set_ylabel('PPLå€¼')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].axhline(y=np.mean(ppls), color='red', linestyle='--', alpha=0.7, label=f'å¹³å‡å€¼: {np.mean(ppls):.2f}')
        axes[0, 0].legend()
        
        # 2. è¯­ä¹‰ç†µ
        entropies = [r['entropy'] for r in rounds_data]
        axes[0, 1].plot(round_numbers, entropies, marker='s', linewidth=2, markersize=6, color=self.colors[1])
        axes[0, 1].set_title('è¯­ä¹‰ç†µ', fontweight='bold')
        axes[0, 1].set_xlabel('è½®æ¬¡')
        axes[0, 1].set_ylabel('ç†µå€¼')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].axhline(y=np.mean(entropies), color='red', linestyle='--', alpha=0.7, label=f'å¹³å‡å€¼: {np.mean(entropies):.2f}')
        axes[0, 1].legend()
        
        # 3. ROUGE-1 F1åˆ†æ•°
        rouge1_f1s = [r['rouge1_f1'] for r in rounds_data]
        axes[0, 2].plot(round_numbers, rouge1_f1s, marker='^', linewidth=2, markersize=6, color=self.colors[2])
        axes[0, 2].set_title('ROUGE-1 F1åˆ†æ•°', fontweight='bold')
        axes[0, 2].set_xlabel('è½®æ¬¡')
        axes[0, 2].set_ylabel('F1åˆ†æ•°')
        axes[0, 2].grid(True, alpha=0.3)
        axes[0, 2].axhline(y=np.mean(rouge1_f1s), color='red', linestyle='--', alpha=0.7, label=f'å¹³å‡å€¼: {np.mean(rouge1_f1s):.3f}')
        axes[0, 2].legend()
        
        # 4. BLEUåˆ†æ•°
        bleus = [r['bleu'] for r in rounds_data]
        axes[1, 0].plot(round_numbers, bleus, marker='d', linewidth=2, markersize=6, color=self.colors[3])
        axes[1, 0].set_title('BLEUåˆ†æ•°', fontweight='bold')
        axes[1, 0].set_xlabel('è½®æ¬¡')
        axes[1, 0].set_ylabel('BLEUåˆ†æ•°')
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].axhline(y=np.mean(bleus), color='red', linestyle='--', alpha=0.7, label=f'å¹³å‡å€¼: {np.mean(bleus):.3f}')
        axes[1, 0].legend()
        
        # 5. è¯æ±‡ä¸°å¯Œåº¦ (TTR)
        ttrs = [r['lex_div_ttr'] for r in rounds_data]
        axes[1, 1].plot(round_numbers, ttrs, marker='p', linewidth=2, markersize=6, color=self.colors[4])
        axes[1, 1].set_title('è¯æ±‡ä¸°å¯Œåº¦ (TTR)', fontweight='bold')
        axes[1, 1].set_xlabel('è½®æ¬¡')
        axes[1, 1].set_ylabel('TTRå€¼')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].axhline(y=np.mean(ttrs), color='red', linestyle='--', alpha=0.7, label=f'å¹³å‡å€¼: {np.mean(ttrs):.3f}')
        axes[1, 1].legend()
        
        # 6. Unigramç†µ
        unigram_entropies = [r['lex_div_unigram_entropy'] for r in rounds_data]
        axes[1, 2].plot(round_numbers, unigram_entropies, marker='h', linewidth=2, markersize=6, color=self.colors[5])
        axes[1, 2].set_title('Unigramç†µ', fontweight='bold')
        axes[1, 2].set_xlabel('è½®æ¬¡')
        axes[1, 2].set_ylabel('ç†µå€¼')
        axes[1, 2].grid(True, alpha=0.3)
        axes[1, 2].axhline(y=np.mean(unigram_entropies), color='red', linestyle='--', alpha=0.7, label=f'å¹³å‡å€¼: {np.mean(unigram_entropies):.2f}')
        axes[1, 2].legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è´¨é‡æŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def plot_capacity_analysis(self, evaluation_data: Dict, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶ä¼ è¾“å®¹é‡åˆ†æå›¾è¡¨
        
        Args:
            evaluation_data: è¯„ä¼°æ•°æ®å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('éšå†™ä¼ è¾“å®¹é‡åˆ†æ', fontsize=16, fontweight='bold')
        
        capacity_metrics = evaluation_data['average_capacity_metrics']
        
        # 1. å®¹é‡æŒ‡æ ‡æŸ±çŠ¶å›¾
        metrics = ['æ¯è½®æ¯”ç‰¹æ•°', 'æ¯æ¯”ç‰¹è½®æ•°']
        values = [capacity_metrics['bits_per_round'], capacity_metrics['round_per_bit']]
        
        bars = axes[0].bar(metrics, values, color=[self.colors[0], self.colors[1]], alpha=0.7, edgecolor='black')
        axes[0].set_title('ä¼ è¾“æ•ˆç‡æŒ‡æ ‡', fontweight='bold')
        axes[0].set_ylabel('æ•°å€¼')
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. è½®æ¬¡ä¸ç´¯ç§¯æ¯”ç‰¹æ•°å…³ç³»
        rounds_data = evaluation_data['rounds']
        round_numbers = [r['round_number'] for r in rounds_data]
        cumulative_bits = [i * capacity_metrics['bits_per_round'] for i in range(1, len(round_numbers) + 1)]
        
        axes[1].plot(round_numbers, cumulative_bits, marker='o', linewidth=2, markersize=6, color=self.colors[2])
        axes[1].set_title('ç´¯ç§¯ä¼ è¾“æ¯”ç‰¹æ•°', fontweight='bold')
        axes[1].set_xlabel('è½®æ¬¡')
        axes[1].set_ylabel('ç´¯ç§¯æ¯”ç‰¹æ•°')
        axes[1].grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(round_numbers, cumulative_bits, 1)
        p = np.poly1d(z)
        axes[1].plot(round_numbers, p(round_numbers), "--", alpha=0.7, color='red', 
                    label=f'è¶‹åŠ¿çº¿ (æ–œç‡: {z[0]:.2f})')
        axes[1].legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å®¹é‡åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def plot_correlation_heatmap(self, evaluation_data: Dict, save_path: Optional[str] = None):
        """
        ç»˜åˆ¶æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾
        
        Args:
            evaluation_data: è¯„ä¼°æ•°æ®å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        rounds_data = evaluation_data['rounds']
        
        # æ„å»ºæ•°æ®çŸ©é˜µ
        metrics_data = {
            'å›°æƒ‘åº¦': [r['ppl'] for r in rounds_data],
            'è¯­ä¹‰ç†µ': [r['entropy'] for r in rounds_data],
            'ROUGE-1 F1': [r['rouge1_f1'] for r in rounds_data],
            'BLEU': [r['bleu'] for r in rounds_data],
            'TTR': [r['lex_div_ttr'] for r in rounds_data],
            'RTTR': [r['lex_div_rttr'] for r in rounds_data],
            'Unigramç†µ': [r['lex_div_unigram_entropy'] for r in rounds_data]
        }
        
        df = pd.DataFrame(metrics_data)
        correlation_matrix = df.corr()
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        plt.title('è¯„ä¼°æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def generate_comprehensive_report(self, evaluation_data: Dict, output_dir: str):
        """
        ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            evaluation_data: è¯„ä¼°æ•°æ®å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆå„ç§å›¾è¡¨
        quality_plot_path = os.path.join(output_dir, "quality_metrics.png")
        capacity_plot_path = os.path.join(output_dir, "capacity_analysis.png")
        correlation_plot_path = os.path.join(output_dir, "correlation_heatmap.png")
        
        self.plot_quality_metrics(evaluation_data, quality_plot_path)
        self.plot_capacity_analysis(evaluation_data, capacity_plot_path)
        self.plot_correlation_heatmap(evaluation_data, correlation_plot_path)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report_path = os.path.join(output_dir, "evaluation_report.html")
        self._generate_html_report(evaluation_data, html_report_path)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_report_path = os.path.join(output_dir, "evaluation_report.md")
        self._generate_markdown_report(evaluation_data, md_report_path)
        
        print(f"ç»¼åˆè¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {output_dir}")
        print(f"- HTMLæŠ¥å‘Š: {html_report_path}")
        print(f"- MarkdownæŠ¥å‘Š: {md_report_path}")
        print(f"- å›¾è¡¨æ–‡ä»¶: {quality_plot_path}, {capacity_plot_path}, {correlation_plot_path}")
    
    def _generate_html_report(self, evaluation_data: Dict, output_path: str):
        """ç”ŸæˆHTMLæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        config = evaluation_data['experiment_config']
        capacity = evaluation_data['average_capacity_metrics']
        quality = evaluation_data['average_quality_metrics']
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A2Aéšå†™é€šä¿¡è¯„ä¼°æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1, h2, h3 {{ color: #2c3e50; }}
        h1 {{ border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        .metric-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .metric-card {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #3498db;
        }}
        .metric-value {{
            font-size: 1.5em;
            font-weight: bold;
            color: #2c3e50;
        }}
        .metric-label {{
            color: #666;
            font-size: 0.9em;
        }}
        .config-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        .config-table th, .config-table td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        .config-table th {{
            background: #f8f9fa;
            font-weight: bold;
        }}
        .chart-container {{
            text-align: center;
            margin: 30px 0;
        }}
        .chart-container img {{
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .timestamp {{
            color: #666;
            font-style: italic;
            text-align: right;
            margin-top: 30px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ”’ A2Aéšå†™é€šä¿¡è¯„ä¼°æŠ¥å‘Š</h1>
        
        <h2>ğŸ“‹ å®éªŒé…ç½®</h2>
        <table class="config-table">
            <tr><th>éšå†™ç®—æ³•</th><td>{config['steganography_algorithm']}</td></tr>
            <tr><th>å¯¹è¯é¢†åŸŸ</th><td>{config['question_domain']}</td></tr>
            <tr><th>é—®é¢˜ç¼–å·</th><td>{config['question_index']}</td></tr>
            <tr><th>æ€»è½®æ•°</th><td>{len(evaluation_data['rounds'])}</td></tr>
        </table>
        
        <h2>ğŸ“Š ä¼ è¾“å®¹é‡æŒ‡æ ‡</h2>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-value">{capacity['bits_per_round']:.3f}</div>
                <div class="metric-label">å¹³å‡æ¯è½®ä¼ è¾“æ¯”ç‰¹æ•°</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{capacity['round_per_bit']:.3f}</div>
                <div class="metric-label">å¹³å‡æ¯æ¯”ç‰¹æ‰€éœ€è½®æ•°</div>
            </div>
        </div>
        
        <h2>ğŸ¯ æ–‡æœ¬è´¨é‡æŒ‡æ ‡</h2>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-value">{quality['ppl']:.2f}</div>
                <div class="metric-label">å¹³å‡å›°æƒ‘åº¦ (PPL)</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{quality['entropy']:.3f}</div>
                <div class="metric-label">å¹³å‡è¯­ä¹‰ç†µ</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{quality['rouge1_f1']:.3f}</div>
                <div class="metric-label">å¹³å‡ROUGE-1 F1</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{quality['bleu']:.3f}</div>
                <div class="metric-label">å¹³å‡BLEUåˆ†æ•°</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{quality['lex_div_ttr']:.3f}</div>
                <div class="metric-label">å¹³å‡è¯æ±‡ä¸°å¯Œåº¦ (TTR)</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{quality['lex_div_unigram_entropy']:.2f}</div>
                <div class="metric-label">å¹³å‡Unigramç†µ</div>
            </div>
        </div>
        
        <h2>ğŸ“ˆ å¯è§†åŒ–åˆ†æ</h2>
        <div class="chart-container">
            <h3>æ–‡æœ¬è´¨é‡æŒ‡æ ‡è¶‹åŠ¿</h3>
            <img src="quality_metrics.png" alt="è´¨é‡æŒ‡æ ‡å›¾è¡¨">
        </div>
        
        <div class="chart-container">
            <h3>ä¼ è¾“å®¹é‡åˆ†æ</h3>
            <img src="capacity_analysis.png" alt="å®¹é‡åˆ†æå›¾è¡¨">
        </div>
        
        <div class="chart-container">
            <h3>æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ</h3>
            <img src="correlation_heatmap.png" alt="ç›¸å…³æ€§çƒ­åŠ›å›¾">
        </div>
        
        <div class="timestamp">
            æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        </div>
    </div>
</body>
</html>
        """
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _generate_markdown_report(self, evaluation_data: Dict, output_path: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        config = evaluation_data['experiment_config']
        capacity = evaluation_data['average_capacity_metrics']
        quality = evaluation_data['average_quality_metrics']
        
        md_content = f"""# ğŸ”’ A2Aéšå†™é€šä¿¡è¯„ä¼°æŠ¥å‘Š

## ğŸ“‹ å®éªŒé…ç½®

| é…ç½®é¡¹ | å€¼ |
|--------|-----|
| éšå†™ç®—æ³• | {config['steganography_algorithm']} |
| å¯¹è¯é¢†åŸŸ | {config['question_domain']} |
| é—®é¢˜ç¼–å· | {config['question_index']} |
| æ€»è½®æ•° | {len(evaluation_data['rounds'])} |

## ğŸ“Š ä¼ è¾“å®¹é‡æŒ‡æ ‡

- **å¹³å‡æ¯è½®ä¼ è¾“æ¯”ç‰¹æ•°**: {capacity['bits_per_round']:.3f}
- **å¹³å‡æ¯æ¯”ç‰¹æ‰€éœ€è½®æ•°**: {capacity['round_per_bit']:.3f}

## ğŸ¯ æ–‡æœ¬è´¨é‡æŒ‡æ ‡

### è‡ªç„¶åº¦æŒ‡æ ‡
- **å¹³å‡å›°æƒ‘åº¦ (PPL)**: {quality['ppl']:.2f}
  - æ•°å€¼è¶Šä½è¡¨ç¤ºæ–‡æœ¬è¶Šè‡ªç„¶
- **å¹³å‡è¯­ä¹‰ç†µ**: {quality['entropy']:.3f}
  - æ•°å€¼é€‚ä¸­è¡¨ç¤ºè¯­ä¹‰ç¡®å®šæ€§è¾ƒå¥½

### ç›¸ä¼¼åº¦æŒ‡æ ‡
- **å¹³å‡ROUGE-1 F1**: {quality['rouge1_f1']:.3f}
  - è¡¡é‡ä¸åŸæ–‡çš„è¯æ±‡é‡å åº¦
- **å¹³å‡BLEUåˆ†æ•°**: {quality['bleu']:.3f}
  - è¡¡é‡ä¸åŸæ–‡çš„n-gramç›¸ä¼¼åº¦

### è¯æ±‡ä¸°å¯Œåº¦æŒ‡æ ‡
- **å¹³å‡TTR**: {quality['lex_div_ttr']:.3f}
  - ç±»å‹-æ ‡è®°æ¯”ç‡ï¼Œè¡¡é‡è¯æ±‡å¤šæ ·æ€§
- **å¹³å‡RTTR**: {quality['lex_div_rttr']:.3f}
  - æ ¹å¼TTRï¼Œæ ‡å‡†åŒ–çš„è¯æ±‡å¤šæ ·æ€§
- **å¹³å‡Unigramç†µ**: {quality['lex_div_unigram_entropy']:.2f}
  - è¯æ±‡åˆ†å¸ƒçš„ä¿¡æ¯ç†µ

## ğŸ“ˆ å¯è§†åŒ–åˆ†æ

### æ–‡æœ¬è´¨é‡æŒ‡æ ‡è¶‹åŠ¿
![è´¨é‡æŒ‡æ ‡å›¾è¡¨](quality_metrics.png)

### ä¼ è¾“å®¹é‡åˆ†æ
![å®¹é‡åˆ†æå›¾è¡¨](capacity_analysis.png)

### æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ
![ç›¸å…³æ€§çƒ­åŠ›å›¾](correlation_heatmap.png)

## ğŸ“ è¯„ä¼°æ€»ç»“

### ä¼˜åŠ¿åˆ†æ
- å›°æƒ‘åº¦: {'è¾ƒä½ï¼Œæ–‡æœ¬è‡ªç„¶åº¦å¥½' if quality['ppl'] < 50 else 'ä¸­ç­‰' if quality['ppl'] < 100 else 'è¾ƒé«˜ï¼Œéœ€è¦æ”¹è¿›'}
- ROUGE-1 F1: {'è¾ƒé«˜ï¼Œä¸åŸæ–‡ç›¸ä¼¼åº¦å¥½' if quality['rouge1_f1'] > 0.5 else 'ä¸­ç­‰' if quality['rouge1_f1'] > 0.3 else 'è¾ƒä½ï¼Œéœ€è¦æ”¹è¿›'}
- ä¼ è¾“æ•ˆç‡: {'è¾ƒé«˜' if capacity['bits_per_round'] > 1 else 'ä¸­ç­‰' if capacity['bits_per_round'] > 0.5 else 'è¾ƒä½'}

### æ”¹è¿›å»ºè®®
1. å¦‚æœå›°æƒ‘åº¦è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–éšå†™ç®—æ³•æˆ–è°ƒæ•´å‚æ•°
2. å¦‚æœç›¸ä¼¼åº¦è¿‡ä½ï¼Œå»ºè®®å¢å¼ºæ–‡æœ¬ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§
3. å¦‚æœä¼ è¾“æ•ˆç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–ç¼–ç ç­–ç•¥

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)


def visualize_evaluation_results(evaluation_json_path: str, output_dir: str = None):
    """
    å¯è§†åŒ–è¯„ä¼°ç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        evaluation_json_path: è¯„ä¼°ç»“æœJSONæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨JSONæ–‡ä»¶æ‰€åœ¨ç›®å½•
    """
    # è¯»å–è¯„ä¼°æ•°æ®
    with open(evaluation_json_path, 'r', encoding='utf-8') as f:
        evaluation_data = json.load(f)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(evaluation_json_path), 'visualization')
    
    # åˆ›å»ºå¯è§†åŒ–å™¨å¹¶ç”ŸæˆæŠ¥å‘Š
    visualizer = EvaluationVisualizer()
    visualizer.generate_comprehensive_report(evaluation_data, output_dir)
    
    return output_dir


def batch_visualize_evaluations(evaluation_dir: str, output_base_dir: str = None):
    """
    æ‰¹é‡å¯è§†åŒ–å¤šä¸ªè¯„ä¼°ç»“æœ
    
    Args:
        evaluation_dir: åŒ…å«è¯„ä¼°JSONæ–‡ä»¶çš„ç›®å½•
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
    """
    if output_base_dir is None:
        output_base_dir = os.path.join(evaluation_dir, 'batch_visualization')
    
    json_files = [f for f in os.listdir(evaluation_dir) if f.endswith('.json') and f.startswith('evaluation_')]
    
    print(f"å‘ç° {len(json_files)} ä¸ªè¯„ä¼°æ–‡ä»¶")
    
    for json_file in json_files:
        print(f"å¤„ç†æ–‡ä»¶: {json_file}")
        json_path = os.path.join(evaluation_dir, json_file)
        output_dir = os.path.join(output_base_dir, json_file.replace('.json', ''))
        
        try:
            visualize_evaluation_results(json_path, output_dir)
            print(f"âœ… å·²å®Œæˆ: {json_file}")
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {json_file}: {e}")
    
    print(f"æ‰¹é‡å¯è§†åŒ–å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {output_base_dir}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¯è§†åŒ–å•ä¸ªè¯„ä¼°ç»“æœ
    # visualize_evaluation_results("path/to/evaluation_result.json")
    
    # ç¤ºä¾‹ï¼šæ‰¹é‡å¯è§†åŒ–
    # batch_visualize_evaluations("path/to/evaluation_results_dir")
    
    pass





